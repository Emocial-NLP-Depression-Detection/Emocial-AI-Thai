{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/nlp-getting-started : NLP Disaster Tweets\n",
    "# df = pd.read_csv(\"train.csv\")\n",
    "df = pd.read_csv(\"./data/data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Unnamed: 0                                             Tweets  label\n",
       "0           0  ไปเป็นประเทศที่แข็งแกร่ง ได้รับการยอมรับจากนาน...      0\n",
       "1           1  ผมได้ตัดสินใจอย่างแน่วแน่ที่จะปรับเปลี่ยนวิธีก...      1\n",
       "2           2  New Normal 3 – “ทำงานเชิงรุก” นายกรัฐมนตรีจะมี...      1\n",
       "3           3  New Normal 2 – “ประเมินผลงานภาครัฐ โดยผู้มีส่ว...      0\n",
       "4           4  New Normal 1 – “ผนึกทุกภาคส่วนร่วมวางอนาคตประเ...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Tweets</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>ไปเป็นประเทศที่แข็งแกร่ง ได้รับการยอมรับจากนาน...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>ผมได้ตัดสินใจอย่างแน่วแน่ที่จะปรับเปลี่ยนวิธีก...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>New Normal 3 – “ทำงานเชิงรุก” นายกรัฐมนตรีจะมี...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>New Normal 2 – “ประเมินผลงานภาครัฐ โดยผู้มีส่ว...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>New Normal 1 – “ผนึกทุกภาคส่วนร่วมวางอนาคตประเ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "101\n99\n"
     ]
    }
   ],
   "source": [
    "print((df.label == 1).sum()) # Disaster\n",
    "print((df.label == 0).sum()) # No Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ผมขอให้สถาบันของท่านมีความมั่นคง พร้อมช่วยกันดูแลและเดินหน้าประเทศไทยของเราให้ผ่านสถานการณ์นี้ไปให้ได้ด้วยกันทั้งประเทศ  https://t.co/5MMxfJN8nj\nt\nผมขอให้สถาบันของท่านมีความมั่นคง พร้อมช่วยกันดูแลและเดินหน้าประเทศไทยของเราให้ผ่านสถานการณ์นี้ไปให้ได้ด้วยกันทั้งประเทศ  \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "for t in df.Tweets:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweets\"] = df.Tweets.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "df[\"Tweets\"] = df.Tweets.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "# pip install nltk\n",
    "from pythainlp.corpus import stopwords\n",
    "\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = stopwords.words(\"thai\")\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tweets\"] = df.Tweets.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in word_tokenize(text):\n",
    "            if word != ' ':\n",
    "                count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(df.Tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1407"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('และ', 256), ('ที่', 206), ('ผม', 178), ('ของ', 136), ('ใน', 126)]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "counter.most_common(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# split text and labels\n",
    "train_sentences = train_df.Tweets.to_numpy()\n",
    "train_labels = train_df.label.to_numpy()\n",
    "val_sentences = val_df.Tweets.to_numpy()\n",
    "val_labels = val_df.label.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((160,), (40,))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_sentences.shape, val_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ผมเดินทางไปพบกับสมาคมธนาคารไทย ได้รับทราบข้อมูลและรับฟังข้อเสนอต่างๆ จากผู้บริหารระดับสูงของธนาคารพาณิชย์ที่เป็นสมาชิกของสมาคม ซึ่งเป็นการพูดคุยที่มีประโยชน์มาก'\n 'the court will appoint professionals to supervise its rehabilitation and restructuring in a professional way i hope that we may see again an airline that thais can be proud about and which can contribute to the prosperity of thailand'\n 'under this courtsupervised rehabilitation process thai airways may continue to fly and its staff still be employed but without the government putting in more money importantly it will now be able to start a muchdelayed restructuring'\n 'let’s think about why we have thai airways thai airways exists to build our country’s reputation and support the prosperity of thais for that it needs to stand on its own feet and compete globally that is the basis on which i made my decision'\n 'that’s why i must save the people’s money for future programmes that directly help them survive and then rebuild their lives and the country’s economy']\n[[132, 133, 134, 135], [1, 136, 23, 137, 138, 2, 139, 24, 25, 3, 35, 6, 16, 140, 63, 4, 141, 7, 14, 36, 142, 143, 37, 144, 7, 38, 31, 26, 145, 39, 3, 40, 31, 146, 2, 1, 41, 5, 64], [147, 32, 42, 25, 148, 11, 12, 36, 149, 2, 150, 3, 24, 151, 43, 26, 152, 17, 153, 1, 154, 155, 6, 44, 18, 156, 13, 23, 157, 26, 158, 2, 159, 16, 160, 35], [161, 162, 39, 45, 14, 163, 11, 12, 11, 12, 164, 2, 165, 46, 47, 166, 3, 65, 1, 41, 5, 38, 21, 7, 13, 167, 2, 168, 27, 24, 169, 170, 3, 171, 172, 7, 15, 1, 173, 27, 40, 4, 28, 174, 48], [49, 45, 4, 175, 176, 1, 177, 18, 21, 178, 179, 7, 180, 29, 181, 182, 3, 183, 184, 185, 186, 3, 1, 47, 187]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[10:15])\n",
    "print(train_sequences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "NAME = \"Emocial-LSTM-{}\".format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\n",
    "    \"logs\",\n",
    "    \"fit\",\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((160, 20), (40, 20))"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "train_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([132, 133, 134, 135,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_padded[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ผมเดินทางไปพบกับสมาคมธนาคารไทย ได้รับทราบข้อมูลและรับฟังข้อเสนอต่างๆ จากผู้บริหารระดับสูงของธนาคารพาณิชย์ที่เป็นสมาชิกของสมาคม ซึ่งเป็นการพูดคุยที่มีประโยชน์มาก\n[132, 133, 134, 135]\n[132 133 134 135   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[10])\n",
    "print(train_sequences[10])\n",
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[132, 133, 134, 135]\nผมเดินทางไปพบกับสมาคมธนาคารไทย ได้รับทราบข้อมูลและรับฟังข้อเสนอต่างๆ จากผู้บริหารระดับสูงของธนาคารพาณิชย์ที่เป็นสมาชิกของสมาคม ซึ่งเป็นการพูดคุยที่มีประโยชน์มาก\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "print(train_sequences[10])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 20, 32)            45024     \n_________________________________________________________________\nlstm (LSTM)                  (None, 128)               82432     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 129       \n=================================================================\nTotal params: 127,585\nTrainable params: 127,585\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(128, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "5/5 [==============================] - 3s 391ms/step - loss: 0.6933 - accuracy: 0.5241 - val_loss: 0.6959 - val_accuracy: 0.4000\n",
      "Epoch 2/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6917 - accuracy: 0.5167 - val_loss: 0.6939 - val_accuracy: 0.4000\n",
      "Epoch 3/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6907 - accuracy: 0.5378 - val_loss: 0.6946 - val_accuracy: 0.4000\n",
      "Epoch 4/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6873 - accuracy: 0.5207 - val_loss: 0.6956 - val_accuracy: 0.4750\n",
      "Epoch 5/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6671 - accuracy: 0.7462 - val_loss: 0.7397 - val_accuracy: 0.4000\n",
      "Epoch 6/30\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5815 - accuracy: 0.8423 - val_loss: 1.2115 - val_accuracy: 0.4000\n",
      "Epoch 7/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.4210 - accuracy: 0.8017 - val_loss: 1.6695 - val_accuracy: 0.4250\n",
      "Epoch 8/30\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3685 - accuracy: 0.8115 - val_loss: 2.3077 - val_accuracy: 0.4250\n",
      "Epoch 9/30\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.3100 - accuracy: 0.8419 - val_loss: 1.6500 - val_accuracy: 0.4250\n",
      "Epoch 10/30\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.3041 - accuracy: 0.8209 - val_loss: 1.8037 - val_accuracy: 0.4250\n",
      "Epoch 11/30\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.2836 - accuracy: 0.8795 - val_loss: 2.2247 - val_accuracy: 0.4250\n",
      "Epoch 12/30\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.2974 - accuracy: 0.8037 - val_loss: 2.0480 - val_accuracy: 0.4250\n",
      "Epoch 13/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2918 - accuracy: 0.7711 - val_loss: 1.7677 - val_accuracy: 0.4250\n",
      "Epoch 14/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2427 - accuracy: 0.8423 - val_loss: 2.2265 - val_accuracy: 0.4250\n",
      "Epoch 15/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2858 - accuracy: 0.8178 - val_loss: 2.6658 - val_accuracy: 0.4250\n",
      "Epoch 16/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2578 - accuracy: 0.8008 - val_loss: 2.5121 - val_accuracy: 0.4250\n",
      "Epoch 17/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3003 - accuracy: 0.7978 - val_loss: 2.2157 - val_accuracy: 0.4250\n",
      "Epoch 18/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2921 - accuracy: 0.7649 - val_loss: 2.4592 - val_accuracy: 0.4250\n",
      "Epoch 19/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2592 - accuracy: 0.8280 - val_loss: 3.0884 - val_accuracy: 0.4250\n",
      "Epoch 20/30\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2635 - accuracy: 0.8265 - val_loss: 3.3853 - val_accuracy: 0.4250\n",
      "Epoch 21/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2565 - accuracy: 0.7959 - val_loss: 3.1277 - val_accuracy: 0.4250\n",
      "Epoch 22/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2437 - accuracy: 0.8159 - val_loss: 2.7719 - val_accuracy: 0.4250\n",
      "Epoch 23/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2664 - accuracy: 0.8155 - val_loss: 2.8681 - val_accuracy: 0.4250\n",
      "Epoch 24/30\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2798 - accuracy: 0.7924 - val_loss: 3.7251 - val_accuracy: 0.4250\n",
      "Epoch 25/30\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.2799 - accuracy: 0.7513 - val_loss: 4.2198 - val_accuracy: 0.4250\n",
      "Epoch 26/30\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2809 - accuracy: 0.8512 - val_loss: 4.3574 - val_accuracy: 0.4250\n",
      "Epoch 27/30\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2800 - accuracy: 0.7958 - val_loss: 4.2650 - val_accuracy: 0.4250\n",
      "Epoch 28/30\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2572 - accuracy: 0.8015 - val_loss: 3.6209 - val_accuracy: 0.4250\n",
      "Epoch 29/30\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.2698 - accuracy: 0.7743 - val_loss: 3.4490 - val_accuracy: 0.4250\n",
      "Epoch 30/30\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2823 - accuracy: 0.7839 - val_loss: 3.7337 - val_accuracy: 0.4250\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e94842788>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model.fit(train_padded, train_labels, epochs=30, validation_data=(val_padded, val_labels),  callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(train_padded)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ผมเดินทางไปพบกับสมาคมธนาคารไทย ได้รับทราบข้อมูลและรับฟังข้อเสนอต่างๆ จากผู้บริหารระดับสูงของธนาคารพาณิชย์ที่เป็นสมาชิกของสมาคม ซึ่งเป็นการพูดคุยที่มีประโยชน์มาก'\n 'the court will appoint professionals to supervise its rehabilitation and restructuring in a professional way i hope that we may see again an airline that thais can be proud about and which can contribute to the prosperity of thailand'\n 'under this courtsupervised rehabilitation process thai airways may continue to fly and its staff still be employed but without the government putting in more money importantly it will now be able to start a muchdelayed restructuring'\n 'let’s think about why we have thai airways thai airways exists to build our country’s reputation and support the prosperity of thais for that it needs to stand on its own feet and compete globally that is the basis on which i made my decision'\n 'that’s why i must save the people’s money for future programmes that directly help them survive and then rebuild their lives and the country’s economy'\n 'we’ve made good progress in controlling the health crisis but the covid crisis is also a financial crisis that’s destroying the livelihoods of everyone and we still don’t know how much worse it will get'\n 'in this time of crisis when everyone’s livelihood is being destroyed by the covid catastrophe we need to keep the nation’s money to help the public in the months ahead to help farmers smes the selfemployed and everyone else trying to earn an honest living'\n 'i had three choices find more money for thai airways or let it go bankrupt or refuse to finance thai airways but put it into courtsupervised rehabilitation i decided on the third option'\n 'twitter today i made a difficult decision regarding thaiairways but it’s one that i know is in the best interests of every member of the public and of our country'\n 'นั่นคือการตัดสินใจของผม และเป็นทิศทางที่รัฐบาลจะยึดปฏิบัติกับกรณีของการบินไทย คือไม่อุ้มนะครับ ต้องเข้าสู่กระบวนการฟื้นฟูภายใต้คำสั่งของศาล ส่วนในรายละเอียดต่างๆ จะเป็นไปตามที่ศาลกำหนด และคาดว่าจะสามารถแจ้งให้ทุกท่านทราบในโอกาสต่อไป']\n[1 0 0 0 1 0 0 0 1 0]\n[1, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[10:20])\n",
    "\n",
    "print(train_labels[10:20])\n",
    "print(predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"th-model.h5\", include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"th-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "\tencoded = [1]\n",
    "\n",
    "\tfor word in s:\n",
    "\t\tif word.lower() in word_index:\n",
    "\t\t\tencoded.append(word_index[word.lower()])\n",
    "\t\telse:\n",
    "\t\t\tencoded.append(2)\n",
    "\n",
    "\treturn encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../test.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e0f16d46e134>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../test.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0mnline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mencode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mencode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# make the data 250 words long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../test.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"../test.txt\", encoding=\"utf-8\") as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\tnline = line.replace(\",\", \"\").replace(\".\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace(\"\\\"\",\"\").strip().split(\" \")\n",
    "\t\tencode = review_encode(nline)\n",
    "\t\tencode = keras.preprocessing.sequence.pad_sequences([encode], padding=\"post\", maxlen=20) # make the data 250 words long\n",
    "\t\tpredict = model.predict(encode)\n",
    "\t\tprint(line)\n",
    "\t\tprint(encode)\n",
    "\t\tprint(round(float(predict[0])))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:cyan\"> |-Default search space size: 1</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">input_units (Int)</h2></span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:cyan\"> |-default: None</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:blue\"> |-max_value: 256</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:cyan\"> |-min_value: 32</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:blue\"> |-sampling: None</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span style=\"color:cyan\"> |-step: 32</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [32,1] vs. [32,20,128]\n\t [[node gradient_tape/binary_crossentropy/mul_1/BroadcastGradientArgs (defined at d:\\SK work\\nsc2021\\venv\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py:95) ]] [Op:__inference_train_function_33727]\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-72615ed74575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mtuner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkerastuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"val_accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutions_per_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_space_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mtuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_padded\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32md:\\SK work\\nsc2021\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [32,1] vs. [32,20,128]\n\t [[node gradient_tape/binary_crossentropy/mul_1/BroadcastGradientArgs (defined at d:\\SK work\\nsc2021\\venv\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py:95) ]] [Op:__inference_train_function_33727]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameter\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import kerastuner\n",
    "LOG_DIR = f\"tune/{int(time.time())}\"\n",
    "def build_model(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "    model.add(layers.LSTM(hp.Int(\"input_units\", min_value=32, max_value=256, step=32), dropout=0.1, return_sequences=True))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    # compile\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    optim = keras.optimizers.Adam(lr=0.001)\n",
    "    metrics = [\"accuracy\"]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "# model = build_model()\n",
    "# model.fit(train_padded, train_labels, epochs=100, validation_data=(val_padded, val_labels), verbose=2)\n",
    "tuner = RandomSearch(build_model, objective=kerastuner.Objective(\"val_accuracy\", direction=\"max\"), max_trials=1, executions_per_trial=1, directory=LOG_DIR)\n",
    "tuner.search_space_summary()\n",
    "tuner.search(x=train_padded,y=train_labels, epochs=30,validation_data=(val_padded, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0bf278b70b7bb17e9cb515820cdf4df218c1a18c5c48580adf480bb6ee1c253b6",
   "display_name": "Python 3.7.6  ('venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "bf278b70b7bb17e9cb515820cdf4df218c1a18c5c48580adf480bb6ee1c253b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}